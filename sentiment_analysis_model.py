# -*- coding: utf-8 -*-
"""Sentiment-analysis-model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JDQHYNRCZntgLpOkbxwUD6_bMJ0u4WZT
"""

import torch
print(torch.version.cuda)  # Should output the installed CUDA version
print(torch.cuda.is_available())  # Should return True
print(torch.cuda.device_count())  # Should show the number of GPUs available

import os
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

!pip install datasets

!pip install transformers

!pip install huggingface_hub

!apt-get install git-lfs

"""# **PREPROCESSING**"""

from datasets import load_dataset

ds = load_dataset("Sp1786/multiclass-sentiment-analysis-dataset")

print(ds)

from datasets import DatasetDict
from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, Trainer, TrainingArguments

# Load your dataset splits
train_dataset = ds["train"]  # Original train split
validation_dataset = ds["validation"]  # Original validation split
test_dataset = ds["test"]  # Original test split

"""# **TOKENIZE THE DS**"""

# from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

def tokenize_function(examples):
    # Ensure 'text' is a string or a list of strings, and handle None values
    texts = examples["text"]
    if texts is None:  # Handle None values
        texts = [""]  # Replace None with empty string
    elif isinstance(texts, str):
        texts = [texts]  # Wrap single string in a list
    # Filter out any non-string elements in the list
    texts = [t for t in texts if isinstance(t, str)]

    return tokenizer(texts, padding="max_length", truncation=True, max_length=512)

# Now proceed with mapping the tokenize_function:
train_dataset = train_dataset.map(tokenize_function, batched=True)
validation_dataset = validation_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batch_size=1000, drop_last_batch=True)

print(train_dataset.column_names)
print(validation_dataset.column_names)
print(test_dataset.column_names)

from transformers import DataCollatorWithPadding
# Prepare the data collator
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# Load the pretrained model
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=3)  # Our dataset contains 3 classes

"""# **TRAINING THE MODEL**"""

!pip install evaluate

"""## COMPUTE METRICS FUNCTION"""

!pip install scikit-learn

import sklearn
print(sklearn.__version__)

import numpy as np
from sklearn.metrics import accuracy_score, f1_score
from evaluate import load

# Define the compute_metrics function
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)

    # Handle single-class edge cases
    if len(set(labels)) == 1:  # Only one class in references
        return {"accuracy": 0.0, "f1": 0.0}  # Fallback values

    # Compute accuracy
    accuracy = accuracy_score(labels, predictions)

    # Compute F1 score manually without relying on `average`
    try:
        f1 = f1_score(labels, predictions, average=None).mean()  # Mean of F1 scores across all classes
    except Exception as e:
        print(f"F1 computation error: {e}")
        f1 = 0.0  # Fallback value

    return {"accuracy": accuracy, "f1": f1}

from huggingface_hub import notebook_login
notebook_login()

!pip install wandb

import wandb
print(wandb.__version__)

# Define training arguments

repo_name = "Sentiment-analysis-tool"

training_args = TrainingArguments(
    run_name = "sentiment_analysis",    #specify a run name
    output_dir= repo_name,              # Directory for saving model checkpoints
    eval_strategy="epoch",              # Evaluate at the end of every epoch
    learning_rate=2e-5,                 # Learning rate
    per_device_train_batch_size=16,     # Batch size for training
    per_device_eval_batch_size=16,      # Batch size for evaluation
    num_train_epochs=3,                 # Number of training epochs
    weight_decay=0.01,                  # Weight decay for regularization
    logging_dir="./logs",               # Directory for logging
    logging_steps=10,                   # Log every 10 steps
    save_total_limit=2,                 # Keep only the last 2 checkpoints
    save_strategy="epoch",              # Save model checkpoints after every epoch
    push_to_hub=True,
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=validation_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,#FROM PREVIOUS CODE
)

# Train the model
wandb.init(project="sentiment-analysis-tool", name="sentiment-analysis") # Initialize wandb run
wandb.login()
trainer.train()

print(test_dataset.column_names)
print(test_dataset[0])  # Look at one example

def flatten_nested(example):
    example["input_ids"] = example["input_ids"][0]
    example["attention_mask"] = example["attention_mask"][0]
    return example

test_dataset = test_dataset.map(flatten_nested)

# Evaluate on the test set
results = trainer.evaluate(test_dataset)
print("Test results:", results)